{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "```\n",
    "pip install langchain\n",
    "pip install openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"random-string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(openai_api_base=\"http://localhost:8000/api/v1\", max_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' Sure! A large language model is a type of artificial intelligence (AI) model that is trained on a massive amount of text data to learn the patterns and structures of human language. The goal of training such a model is to enable it to generate natural-sounding, coherent text when given a prompt or input.\\n\\nLarge language models are typically made up of many layers of artificial neural networks, which are designed to mimic the way that the human brain processes information. By using these networks, the model can learn to predict the next word in a sentence, or even generate entirely new sentences based on what it has learned from the training data.\\n\\nSome examples of large language models include GPT-3 (Generative Pre-trained Transformer 3), BERT (Bidirectional Encoder Representations from Transformers), and RoBERTa (Rotated Bilingual Encoder Representations with Attention). These models have been trained on a massive amount of text data, such as books, articles, and web pages, to learn the patterns and structures of human language.\\n\\nOverall, large language models are powerful tools for natural language processing tasks like text generation, summarization, and translation. They have many', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    HumanMessage(content=\"Can you explain what is a large language model?\")\n",
    "]\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
