name: CI

env:
  DOCKER_BUILDKIT: 1
  DOCKER_CLI_EXPERIMENTAL: "enabled"
  COMPOSE_DOCKER_CLI_BUILD: 1

on:
  pull_request:
    branches: ["main"]

  push:
    branches: ["main"]

jobs:
  linter:
    runs-on: gpu
    steps:
      - name: Checkout Code Repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Run pre-commit
        uses: pre-commit/action@v3.0.0

  pytest:
    runs-on: gpu
    needs: [linter]
    steps:
      - name: Checkout Code Repository
        uses: actions/checkout@v3

      - name: Build the images
        run: docker buildx build --file ./prem-chat-llama-cpp/docker/m1/Dockerfile --build-arg="MODEL_ID=vicuna-7b-q4" --tag prem-chat-llama-cpp ./prem-chat-llama-cpp

      - name: Run the tests
        run: docker run --rm --name prem-chat-vicuna-7b-q4-m1 prem-chat-llama-cpp pytest

      - name: Prune
        run: docker system prune -a -f

      - name: Build the images
        run: docker buildx build --file ./prem-chat-dolly-v2/docker/gpu/Dockerfile --build-arg="MODEL_ID=dolly-v2-12b" --tag prem-chat-dolly-v2 ./prem-chat-dolly-v2

      - name: Run the tests
        run: docker run -it --rm --gpus all --name prem-chat-dolly-v2-12b --env-file ./prem-chat-dolly-v2/.env prem-chat-dolly-v2 pytest

  push:
    runs-on: gpu
    needs: [pytest]
    permissions:
      contents: read
      packages: write
    if: github.event_name == 'push'
    steps:
      - uses: actions/checkout@v2

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v1

      - name: Set up Docker Buildx
        id: buildx
        uses: docker/setup-buildx-action@v2
        with:
          install: true

      - name: Login to GitHub Container Registry
        uses: docker/login-action@v2
        with:
          registry: ghcr.io
          username: ${{ github.repository_owner }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and Push the Images
        run: |
          docker buildx build \
            --push \
            --file ./prem-chat-llama-cpp/docker/m1/Dockerfile \
            --build-arg="MODEL_ID=vicuna-7b-q4" \
            --tag ghcr.io/premai-io/prem-chat-vicuna-7b-q4-m1:latest \
            --platform linux/arm64,linux/amd64 ./prem-chat-llama-cpp

          docker system prune -f -a

          docker buildx build \
            --push \
            --file ./prem-chat-llama-cpp/docker/m1/Dockerfile \
            --build-arg="MODEL_ID=gpt4all-lora-q4" \
            --tag ghcr.io/premai-io/prem-chat-gpt4all-lora-q4-m1:latest \
            --platform linux/arm64,linux/amd64 ./prem-chat-llama-cpp

          docker system prune -f -a

          docker buildx build \
            --push \
            --file ./prem-chat-dolly-v2/docker/gpu/Dockerfile \
            --build-arg="MODEL_ID=dolly-v2-12b" \
            --tag ghcr.io/premai-io/prem-chat-dolly-v2-12b-gpu:latest \
            --platform linux/arm64,linux/amd64 ./dolly-v2-12b
