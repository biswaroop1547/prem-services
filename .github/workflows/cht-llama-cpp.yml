name: Chat Llama.cpp

env:
  DOCKER_BUILDKIT: 1
  DOCKER_CLI_EXPERIMENTAL: "enabled"
  COMPOSE_DOCKER_CLI_BUILD: 1

on:
  pull_request:
    branches: ["main"]
    paths:
      - "cht-llama-cpp/**"

  push:
    branches: ["main"]
    paths:
      - "cht-llama-cpp/**"

jobs:
  test-cht-llama-cpp:
    runs-on: gpu
    permissions:
        contents: read
        packages: write
    steps:
    - uses: actions/checkout@v3

    - name: Login to GitHub Container Registry
      uses: docker/login-action@v2
      with:
        registry: ghcr.io
        username: ${{ github.repository_owner }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Pull the latest images for caching
      run: |
        docker pull ghcr.io/premai-io/chat-vicuna-7b-q4-cpu:latest  || true
        docker pull ghcr.io/premai-io/chat-gpt4all-lora-q4-cpu:latest || true

    - name: Build and Test
      run: |
        docker build --pull \
          --cache-from ghcr.io/premai-io/chat-vicuna-7b-q4-cpu:latest \
          --file ./cht-llama-cpp/docker/cpu/Dockerfile \
          --build-arg="MODEL_ID=vicuna-7b-q4" \
          --tag cht-llama-cpp ./cht-llama-cpp
    - name: Test
      run: |
        docker run --rm cht-llama-cpp pytest

  push-cht-llama-cpp:
    runs-on: gpu
    needs: [test-cht-llama-cpp]
    permissions:
        contents: read
        packages: write
    if: github.event_name == 'push'
    steps:
    - uses: actions/checkout@v3

    - name: Set up QEMU
      uses: docker/setup-qemu-action@v1

    - name: Set up Docker Buildx
      id: buildx
      uses: docker/setup-buildx-action@v2
      with:
        install: true

    - name: Login to GitHub Container Registry
      uses: docker/login-action@v2
      with:
        registry: ghcr.io
        username: ${{ github.repository_owner }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Build and Push
      run: |
        docker buildx create --use
        docker buildx build --pull \
          --cache-from ghcr.io/premai-io/chat-vicuna-7b-q4-cpu:latest \
          --file ./cht-llama-cpp/docker/cpu/Dockerfile \
          --build-arg="MODEL_ID=vicuna-7b-q4" \
          --tag ghcr.io/premai-io/chat-vicuna-7b-q4-cpu:latest \
          --platform linux/arm64,linux/amd64 ./cht-llama-cpp
        docker buildx build --pull \
          --cache-from ghcr.io/premai-io/chat-gpt4all-lora-q4-cpu:latest \
          --file ./cht-llama-cpp/docker/cpu/Dockerfile \
          --build-arg="MODEL_ID=gpt4all-lora-q4" \
          --tag ghcr.io/premai-io/chat-gpt4all-lora-q4-cpu:latest \
          --platform linux/arm64,linux/amd64 ./cht-llama-cpp
