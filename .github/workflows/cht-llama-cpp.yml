name: Chat Llama CPP

env:
  VERSION: 1.0.0
  DOCKER_BUILDKIT: 1
  DOCKER_CLI_EXPERIMENTAL: "enabled"
  COMPOSE_DOCKER_CLI_BUILD: 1

on:
  push:
    branches: ["main"]
    paths:
      - "cht-llama-cpp/**"

jobs:
  build-test-push-cht-llama-cpp:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3

    - name: Set up QEMU
      uses: docker/setup-qemu-action@v2

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2

    - name: Login to GitHub Container Registry
      uses: docker/login-action@v2
      with:
        registry: ghcr.io
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}

    - name: Build and push
      uses: docker/build-push-action@v4
      with:
        context: ./cht-llama-cpp
        push: true
        file: ./cht-llama-cpp/docker/cpu/Dockerfile
        platforms: linux/amd64,linux/arm64
        build-args: MODEL_ID=vicuna-7b-q4
        tags: ghcr.io/premai-io/chat-vicuna-7b-q4-cpu:latest,ghcr.io/premai-io/chat-vicuna-7b-q4-cpu:${{ env.VERSION }}
        cache-from: type=registry,ref=ghcr.io/premai-io/chat-vicuna-7b-q4-cpu:latest

    - name: Build and push
      uses: docker/build-push-action@v4
      with:
        context: ./cht-llama-cpp
        push: true
        file: ./cht-llama-cpp/docker/cpu/Dockerfile
        platforms: linux/amd64,linux/arm64
        build-args: MODEL_ID=gpt4all-lora-q4
        tags: ghcr.io/premai-io/chat-gpt4all-lora-q4-cpu:latest,ghcr.io/premai-io/chat-gpt4all-lora-q4-cpu:${{ env.VERSION }}
        cache-from: type=registry,ref=ghcr.io/premai-io/chat-gpt4all-lora-q4-cpu:latest
