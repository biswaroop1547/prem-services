name: Chat Dolly v2

env:
  VERSION: 0.0.5
  DOCKER_BUILDKIT: 1
  DOCKER_CLI_EXPERIMENTAL: "enabled"
  COMPOSE_DOCKER_CLI_BUILD: 1

on:
  push:
    branches: ["main"]
    paths:
      - "ebd-all-minilm/**"

jobs:
  build-test-push-ebd-all-minilm:
    runs-on: self-hosted
    steps:
    - uses: actions/checkout@v3

    - name: Login to GitHub Container Registry
      uses: docker/login-action@v2
      with:
        registry: ghcr.io
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: Build & Push the Images
      run: |
        docker buildx build --push \
            --cache-from ghcr.io/premai-io/embeddings-all-minilm-l6-v2-cpu:latest \
            --file ./ebd-all-minilm/docker/cpu/Dockerfile \
            --build-arg="MODEL_ID=all-MiniLM-L6-v2" \
            --tag ghcr.io/premai-io/embeddings-all-minilm-l6-v2-cpu:latest \
            --tag ghcr.io/premai-io/embeddings-all-minilm-l6-v2-cpu:$VERSION \
            --platform linux/arm64,linux/amd64 ./ebd-all-minilm

        docker buildx build --push \
            --cache-from ghcr.io/premai-io/embeddings-all-minilm-l6-v2-gpu:latest \
            --file ./ebd-all-minilm/docker/gpu/Dockerfile \
            --build-arg="MODEL_ID=all-MiniLM-L6-v2" \
            --tag ghcr.io/premai-io/embeddings-all-minilm-l6-v2-gpu:latest \
            --tag ghcr.io/premai-io/embeddings-all-minilm-l6-v2-gpu:$VERSION \
            --platform linux/amd64 ./ebd-all-minilm

    - name: Test the Images
      run: |
        docker run --rm --gpus all ghcr.io/premai-io/embeddings-all-minilm-l6-v2-gpu:latest pytest
        docker run --rm ghcr.io/premai-io/embeddings-all-minilm-l6-v2-cpu:latest pytest
